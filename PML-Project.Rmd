---
title: "Practical Machine Learning - Class Project"  
output:
  html_document:
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
---

## Summary  
The purpose of this project is to train a machine learning model with an exercise dataset. The dataset is from a <a href="http://groupware.les.inf.puc-rio.br/har">human activity recognition study</a> where subjects are asked do a series of exercises with a dumbbell in 5 different ways (classes A, B, C, D, and E), only one of which (class A) is the correct way of doing the specific exercise. The machine learning model is then used to predict the class of a testing dataset of 20 observations.

## Data Cleaning
We first import the training and testing datasets.
```{r, cache=TRUE}
training <- read.csv('pml-training.csv', stringsAsFactors=FALSE)
testing <- read.csv('pml-testing.csv', stringsAsFactors=FALSE)
dim(training)
```

We remove these variables from the dataset:  
- X  
- raw_timestamp_part_1  
- raw_timestam_part2  
- cvtd_timestamp  
Since these are just timestamp data and record numering that will not help in the prediction.

```{r removeTimestamp, cache=TRUE}
testing[,c(1,3,4,5)] <- list(NULL)
training[,c(1,3,4,5)] <- list(NULL)
```

Next we proceed to check for NA or blank values. We find that 100 of the remaining 156 variables have 19216 NA values each.  

```{r removeNAs, cache=TRUE}
NACount <- data.frame(column=colnames(training))
naCounter <- data.frame()
library(stringr)
for(i in 1:dim(training)[2]){
      curCount <- sum(is.na(training[,i]) | str_trim(as.character(training[,i])) == "")
      naCounter <- rbind(naCounter, curCount)
}
NACount <- cbind(NACount, naCounter)
colnames(NACount)[2] <- 'NAcount'
library(reshape2)
dcast(NACount, NAcount~'colCount', length, value.var='column')
```

Notice that the NAs are a 97.9% (19216/19622) of the total observations in the training dataset. We further remove these 100 variables, as they are unlikely to have any predictive power, leaving 55 independent variables plus the dependent variable *classe* in the dataset.  

```{r remove_NA_cols, cache=TRUE}
training <- subset(training, select=(NACount$NAcount<1000))
dim(training)
```

## Classification Modeling with Random Forest  

We tested different classification algorithms and found that random forest classification to be the most accurate for the given dataset. We tested three different Random Forest classification models by varying the variables to be included. Since we suspect that the variables *user_name, new_window,* and *num_window* may not have much to do with how the subjects do their exercise, we test different models by excluding these variables as follows:  
- model1: excluding *user_name, new_window,* and *num_window*    
- model2: excluding *new_window*, and *num_window*    
- model3: excluding *user_name*, and *new_window*  

We train the three models with 5-fold cross-validation.  
```{r randomForest, cache=TRUE, message=FALSE}
training$classe <- factor(training$classe)
training$user_name <- factor(training$user_name)
training$new_window <- factor(training$new_window)
library(caret)
set.seed(123)
trControl <- trainControl(method='cv',number=5)
# no window or user_name
model11 <- train(classe~.,method='rf',data=subset(training,select=-c(1:3)),trControl=trControl,ntree=200)
# no window but with user_name
model2 <- train(classe~.,method='rf',data=subset(training,select=-c(2:3)),trControl=trControl,ntree=200)
# with num_window
model3 <- train(classe~.,method='rf',data=subset(training,select=-c(1:2)),trControl=trControl,ntree=200)
```

Notice that the best performing model is *model3*, the one where we exclude *user_name* and *new_window* from the dataset. Below is the summary output of the three different models.
```{r modelPerf}
model11$finalModel;
model2$finalModel;
model3$finalModel;
```  

## Expected Out of Sample Error Rate from Cross-validation  
In random forest classification models, the out-of-bag (OOB) estimate of error rate is the expected out of sample error. The following list summarizes the expected out of sample error from the 5-fold cross-validation of each model:  
- model1: expected OOB error rate = 0.40%  
- model2: expected OOB error rate = 0.45%  
- model3: expected OOB error rate = 0.14%  

The error rate of the models are plotted below as a function of total number of trees. We can see that the error in $model3$ is reduced the quickest as a function of tree numbers.
```{r plotModel3}
par(mfrow=c(3, 1))
plot(model1$finalModel)
plot(model2$finalModel)
plot(model3$finalModel)
```

## Prediction

Next we use the three fitted models to predict the classes of the testing dataset. We also compare the results of these models. Although the three models had slightly different expected OOB error rates, all of them had the same predicted results (see predicted data table below), which increases our confidence in our predictions. 

```{r prediction, cache=TRUE}
# predict with all 3 models
testing$user_name <- factor(testing$user_name)
testing$new_window <- factor(testing$new_window)
predicted <- data.frame(model1=predict(model1,newdata=testing))
predicted$model2 <- predict(model2,newdata=testing)
predicted$model3 <- predict(model3,newdata=testing)
#predicted data table
predicted
```

Finally, we save the results into files for submission.
```{r saveResult, eval=FALSE}
# write prediction to files
for(i in 1:dim(predicted)[1]){
      filename <- paste0("problem_id_",i,".txt")
      write.table(predicted[i,1],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
```